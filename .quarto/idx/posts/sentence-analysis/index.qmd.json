{"title":"Text Classification for Academic Writing","markdown":{"yaml":{"title":"Text Classification for Academic Writing","date":"2024-11-09","date-modified":"2024-11-09","categories":["LLM","Python","R","Web Scraping"],"image":"sunset5.jpeg"},"headingText":"Web Scraping","containsRefs":false,"markdown":"\n\nAcademic journals require research articles to follow a specific format.\nFirst, a 150-300 word abstract provides a big-picture overview of the main ideas contained within the paper.\nFollowing the abstract is a longer introduction.\nThis section serves multiple purposes, including explaining the motivations behind the research, summarizing past results in the field, identifying key gaps in the literature, and providing an overview of key methods and results.\nAfter the introduction, most research articles have dedicated methods, results, discussion, and conclusion sections.\nHowever, these sections are often renamed, split up, or removed entirely to better suit the paper.\n\nSince the abstract and introduction are central to all journal articles, understanding how to write these sections effectively is an essential skill for researchers.\nHowever, advice on paper writing is often general and unclear.\nWhile recommendations like \"start broad and then gradually get narrower\" or \"make sure to emphasize the importance of your research\" aren't bad advice, there's much more nuance in an excellent paper.\nToday, I outline a data-driven framework for writing abstracts and introductions for research articles.\nTo do this, I analyzed 500 papers from the [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/) journal using the open source LLM [Llama 3.2](https://www.llama.com/). \n\n\nThis project required abstracts and introductions from many academic papers. Getting a bunch of abstracts is straightforward, the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) contains millions of them. \nHowever, sourcing the introductions from these papers is much harder.\nMy first idea was to feed the full-text PDFs from the arXiv dataset directly into the LLM.\nHowever, even state-of-the-art models like ChatGPT 4o struggled to differentiate between the various sections in the research articles.\nI'm not entirely sure why this task is so difficult, but I think it might have something to do with the two-column formatting in many academic journals or the weird placement of figures throughout the text.\nI needed a better solution.\n\nLuckily, most journals publish research articles on the internet in a relatively consistent format.\nThis makes it possible to build a web scraper to automatically extract papers from the publisher's website.\nBut web scraping is a tedious and annoying task.\nSmall changes between web pages can completely break your scraper, and you're constantly running the risk of getting your IP permanently banned for making too many requests.\nMany journals have their articles locked behind paywalls, which adds a layer of complexity I wasn't equipped to deal with.\nTo keep things as simple as possible, I extracted papers from a single open-source journal[^1], [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/).\nTo build the web scraper, I used the Python library [beautifulsoup4](https://pypi.org/project/beautifulsoup4/).\nAfter six hours of scraping, I managed to wrangle all 9,653 abstracts and introductions from PLOS Computational Biology into a `.json` file.\nNow I was ready to categorize.\n\n[^1]: I picked computational biology over another PLOS journal because its closest to my own research interests. Hopefully, the results of this analysis will come in handy if I ever manage to write a paper!\n\n## Sentence Categorization\n\nAfter gathering the data, I ran each abstract and introduction through the [Llama 3.2](https://www.llama.com/) 90B text-preview model to categorize the sentences.\nI was hoping to use the 1B and 3B models which are small enough to run on my laptop, but unfortunately, they weren't giving me accurate results.\nSince I don't own a massive stack of GPUs, I used [Groq](https://groq.com/) to run the categorization job in the cloud.\nWith their free plan, I got access to 500,000 tokens of chat completions per day.\nThis was enough to categorize the abstracts and introductions from about 150 papers each night[^2].\nSince doing all 9,653 papers would have taken months, I opted to analyze the 500 most recent papers instead.\nThis was still a big job.\nIn total, the LLM analyzed over 20,000 sentences with over 300,000 words.\nThe prompt I used is shown below: \n\n>Here are four categories for classifying the sentences of a scientific paper.\n>Make sure that you correctly divide the text into sentences.\n>Remember that abbreviations like \"et al.\" do not constitute the end of a sentence.\n>However, all sentences must end with a period or question mark.\n>\n>\n>CATEGORY 1: Motivation for the research in a broad context. \n>This type of sentence helps the reader to understand why the research is relevant, useful, and interesting.\n>\n>CATEGORY 2: Past research in the field, which may be experimental or theoretical. \n>Sentences in this category may also describe the results of previous research efforts.\n>\n>CATEGORY 3: Descriptions of topics that remain poorly understood or descriptions of gaps in the literature that need to be filled.\n>\n>CATEGORY 4: The methods used by the authors in this research paper.\n>Sentences in this category may also discuss the results of this research paper.\n>\n>A sentence can only belong to one category.\n>Use the context of the paragraph to determine whether a sentence is discussing the research paper itself or a previous study.\n>Your response should be a JSON object in the following format:\n>\n>```json\n>{\n>  \"abstract\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ],\n>  \"introduction\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ]\n>}\n>```\n>\n>\n>The \"sentence\" field must contain the exact sentence that was classified. The \"position\" is the relative position of the sentence in the text (i.e. 1 for the first sentence, 2 for the second sentence, etc.). The \"category\" should be 1, 2, 3, or 4. Now, I will provide you with an abstract followed by an introduction. Classify each of the sentences. Ensure that the first object in the \"abstract\" and \"introduction\" fields have a position of 1.\n\n[^2]: While I was running my classification jobs through Groq, I stumbled across the hilariously named and immensely practical `caffeinate` command, which allows you to prevent your computer from sleeping until a specified process completes.\n\n## Results\n\n![**Figure 1**: Smoothed frequency of each sentence category plotted against relative position in the abstract/introduction. Precisely, a sentence at position $m$ in a section with $n$ sentences has a relative position of $m/n$. On average, the categories are arranged in order. Categories 2 (background) and 3 (research gaps) overlap significantly. This suggests that many authors are interweaving past results from the literature with areas for further study.](category-kde.png)\n\n![**Figure 2**: Boxplot of the number of sentences in each section and category. On average, abstracts have two sentences of motivation, two sentences of background, and one sentence explaining a research gap. However, most of the abstract is dedicated to summarizing the methods and results from the paper. In the introduction, authors only include a few sentences about motivation and research gaps. The majority of the introduction is dedicated to explaining background information, with some room for summarizing the methods and results. ](category-count.png)\n\n\n![**Figure 3**: Boxplot of the total number of sentences in the abstract and introduction. The median abstract contains 9 sentences, with most abstracts containing between 6-11 sentences. The vast majority of abstracts are between 150 and 300 words. Introductions tend to have around 35 sentences, although some are significantly longer. Most introductions have about 1,000 words.](section-count.png)\n\n[^3]: For the data anlysis component of this project I tried using the [tidyverse](https://www.tidyverse.org/) for the first time, and it was great! The entire data processing pipeline is just so idiomatic and simple. And `ggplot2` definitely outshines both `matplotlib` and `plotly`. It doesn't seem like R is very fast or scaleable, but I'll definitely be using it for EDA and plotting from now on.\n\n## Conclusions\n\nA standard abstract in PLOS Computational Biology has two sentences of motivation, followed by 2-3 sentences explaining background information and the research gap the authors intend to fill.\nThe rest of the abstract summarizes the methods and results used in the paper.\nA standard introduction contains a few sentences of motivation, followed by 20-30 sentences (3-4 paragraphs) explaining relevant background information and discussing the research gaps.\nThis is followed by 4-5 sentences (1 paragraph) summarizing the paper's methods and results[^3].\n\nI suspect that the structure of academic papers varies between disciplines, so I'm not sure how generalizable these results are.\nHowever, you can find all the code I used on my [Github](https://github.com/rileywheadon/paper-analysis) if you would like to try this on a different journal.\nI also included the abstracts and introductions (~9,000 entries) and the categorized sentences from the 500 papers I analyzed (~20,000 entries) if you'd like to see the data for yourself.\nThanks for reading!\n\n## Acknowledgment\n\nI would like to thank [Prof. Eric Cytrynbaum](https://personal.math.ubc.ca/~cytryn/index.shtml) for providing the idea for this project. \n","srcMarkdownNoYaml":"\n\nAcademic journals require research articles to follow a specific format.\nFirst, a 150-300 word abstract provides a big-picture overview of the main ideas contained within the paper.\nFollowing the abstract is a longer introduction.\nThis section serves multiple purposes, including explaining the motivations behind the research, summarizing past results in the field, identifying key gaps in the literature, and providing an overview of key methods and results.\nAfter the introduction, most research articles have dedicated methods, results, discussion, and conclusion sections.\nHowever, these sections are often renamed, split up, or removed entirely to better suit the paper.\n\nSince the abstract and introduction are central to all journal articles, understanding how to write these sections effectively is an essential skill for researchers.\nHowever, advice on paper writing is often general and unclear.\nWhile recommendations like \"start broad and then gradually get narrower\" or \"make sure to emphasize the importance of your research\" aren't bad advice, there's much more nuance in an excellent paper.\nToday, I outline a data-driven framework for writing abstracts and introductions for research articles.\nTo do this, I analyzed 500 papers from the [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/) journal using the open source LLM [Llama 3.2](https://www.llama.com/). \n\n## Web Scraping\n\nThis project required abstracts and introductions from many academic papers. Getting a bunch of abstracts is straightforward, the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) contains millions of them. \nHowever, sourcing the introductions from these papers is much harder.\nMy first idea was to feed the full-text PDFs from the arXiv dataset directly into the LLM.\nHowever, even state-of-the-art models like ChatGPT 4o struggled to differentiate between the various sections in the research articles.\nI'm not entirely sure why this task is so difficult, but I think it might have something to do with the two-column formatting in many academic journals or the weird placement of figures throughout the text.\nI needed a better solution.\n\nLuckily, most journals publish research articles on the internet in a relatively consistent format.\nThis makes it possible to build a web scraper to automatically extract papers from the publisher's website.\nBut web scraping is a tedious and annoying task.\nSmall changes between web pages can completely break your scraper, and you're constantly running the risk of getting your IP permanently banned for making too many requests.\nMany journals have their articles locked behind paywalls, which adds a layer of complexity I wasn't equipped to deal with.\nTo keep things as simple as possible, I extracted papers from a single open-source journal[^1], [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/).\nTo build the web scraper, I used the Python library [beautifulsoup4](https://pypi.org/project/beautifulsoup4/).\nAfter six hours of scraping, I managed to wrangle all 9,653 abstracts and introductions from PLOS Computational Biology into a `.json` file.\nNow I was ready to categorize.\n\n[^1]: I picked computational biology over another PLOS journal because its closest to my own research interests. Hopefully, the results of this analysis will come in handy if I ever manage to write a paper!\n\n## Sentence Categorization\n\nAfter gathering the data, I ran each abstract and introduction through the [Llama 3.2](https://www.llama.com/) 90B text-preview model to categorize the sentences.\nI was hoping to use the 1B and 3B models which are small enough to run on my laptop, but unfortunately, they weren't giving me accurate results.\nSince I don't own a massive stack of GPUs, I used [Groq](https://groq.com/) to run the categorization job in the cloud.\nWith their free plan, I got access to 500,000 tokens of chat completions per day.\nThis was enough to categorize the abstracts and introductions from about 150 papers each night[^2].\nSince doing all 9,653 papers would have taken months, I opted to analyze the 500 most recent papers instead.\nThis was still a big job.\nIn total, the LLM analyzed over 20,000 sentences with over 300,000 words.\nThe prompt I used is shown below: \n\n>Here are four categories for classifying the sentences of a scientific paper.\n>Make sure that you correctly divide the text into sentences.\n>Remember that abbreviations like \"et al.\" do not constitute the end of a sentence.\n>However, all sentences must end with a period or question mark.\n>\n>\n>CATEGORY 1: Motivation for the research in a broad context. \n>This type of sentence helps the reader to understand why the research is relevant, useful, and interesting.\n>\n>CATEGORY 2: Past research in the field, which may be experimental or theoretical. \n>Sentences in this category may also describe the results of previous research efforts.\n>\n>CATEGORY 3: Descriptions of topics that remain poorly understood or descriptions of gaps in the literature that need to be filled.\n>\n>CATEGORY 4: The methods used by the authors in this research paper.\n>Sentences in this category may also discuss the results of this research paper.\n>\n>A sentence can only belong to one category.\n>Use the context of the paragraph to determine whether a sentence is discussing the research paper itself or a previous study.\n>Your response should be a JSON object in the following format:\n>\n>```json\n>{\n>  \"abstract\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ],\n>  \"introduction\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ]\n>}\n>```\n>\n>\n>The \"sentence\" field must contain the exact sentence that was classified. The \"position\" is the relative position of the sentence in the text (i.e. 1 for the first sentence, 2 for the second sentence, etc.). The \"category\" should be 1, 2, 3, or 4. Now, I will provide you with an abstract followed by an introduction. Classify each of the sentences. Ensure that the first object in the \"abstract\" and \"introduction\" fields have a position of 1.\n\n[^2]: While I was running my classification jobs through Groq, I stumbled across the hilariously named and immensely practical `caffeinate` command, which allows you to prevent your computer from sleeping until a specified process completes.\n\n## Results\n\n![**Figure 1**: Smoothed frequency of each sentence category plotted against relative position in the abstract/introduction. Precisely, a sentence at position $m$ in a section with $n$ sentences has a relative position of $m/n$. On average, the categories are arranged in order. Categories 2 (background) and 3 (research gaps) overlap significantly. This suggests that many authors are interweaving past results from the literature with areas for further study.](category-kde.png)\n\n![**Figure 2**: Boxplot of the number of sentences in each section and category. On average, abstracts have two sentences of motivation, two sentences of background, and one sentence explaining a research gap. However, most of the abstract is dedicated to summarizing the methods and results from the paper. In the introduction, authors only include a few sentences about motivation and research gaps. The majority of the introduction is dedicated to explaining background information, with some room for summarizing the methods and results. ](category-count.png)\n\n\n![**Figure 3**: Boxplot of the total number of sentences in the abstract and introduction. The median abstract contains 9 sentences, with most abstracts containing between 6-11 sentences. The vast majority of abstracts are between 150 and 300 words. Introductions tend to have around 35 sentences, although some are significantly longer. Most introductions have about 1,000 words.](section-count.png)\n\n[^3]: For the data anlysis component of this project I tried using the [tidyverse](https://www.tidyverse.org/) for the first time, and it was great! The entire data processing pipeline is just so idiomatic and simple. And `ggplot2` definitely outshines both `matplotlib` and `plotly`. It doesn't seem like R is very fast or scaleable, but I'll definitely be using it for EDA and plotting from now on.\n\n## Conclusions\n\nA standard abstract in PLOS Computational Biology has two sentences of motivation, followed by 2-3 sentences explaining background information and the research gap the authors intend to fill.\nThe rest of the abstract summarizes the methods and results used in the paper.\nA standard introduction contains a few sentences of motivation, followed by 20-30 sentences (3-4 paragraphs) explaining relevant background information and discussing the research gaps.\nThis is followed by 4-5 sentences (1 paragraph) summarizing the paper's methods and results[^3].\n\nI suspect that the structure of academic papers varies between disciplines, so I'm not sure how generalizable these results are.\nHowever, you can find all the code I used on my [Github](https://github.com/rileywheadon/paper-analysis) if you would like to try this on a different journal.\nI also included the abstracts and introductions (~9,000 entries) and the categorized sentences from the 500 papers I analyzed (~20,000 entries) if you'd like to see the data for yourself.\nThanks for reading!\n\n## Acknowledgment\n\nI would like to thank [Prof. Eric Cytrynbaum](https://personal.math.ubc.ca/~cytryn/index.shtml) for providing the idea for this project. \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.6","theme":"litera","title-block-banner":true,"title":"Text Classification for Academic Writing","date":"2024-11-09","date-modified":"2024-11-09","categories":["LLM","Python","R","Web Scraping"],"image":"sunset5.jpeg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}