{"title":"A Structural Analysis of Academic Writing","markdown":{"yaml":{"title":"A Structural Analysis of Academic Writing","date":"2024-11-05","date-modified":"2024-11-05","categories":["LLM","Python","R","Web Scraping"],"image":"sunset5.jpeg"},"headingText":"Web Scraping","containsRefs":false,"markdown":"\n\nAcademic journals require research articles to follow a specific format. First, an 150-300 word abstract provides an big-picture overview of the main ideas contained within the paper. An abstract may also provide a brief summary of relevant background information or methods used in the paper. Following the abstract, authors typically provide a longer introduction section. In this authors may do a number of things, including explain why their research is important, summarize past results in the field, identify key gaps in the literature, and discuss methods used in the paper. After the introduction, authors typically include a methods, results, discussion, and conclusion section. However, these sections are often modified, split up, or removed entirely to better suit the project.\n\nSince the abstract and introduction are crucial to any research paper, understanding how to write these sections effectively is an essential skill for a researcher. Advice on paper writing is typically *qualitative* -- stuff like \"start broad and then gradually get narrower\" or \"make sure to emphasize the importance of your research\". Today, I outline a *quantitative* framework for writing abstracts and introduction. As an example, my analysis gives an estimate of how many sentences of motivation you should provide and how where these sentences should be located in your paper. To do this, I analyzed over $9\\,000$ papers from the [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/) journal using the open source LLM [Llama 3.2](https://www.llama.com/).\n \n**Remark**: The idea for this project came from my supervisor, [Prof. Eric Cytrynbaum](https://personal.math.ubc.ca/~cytryn/index.shtml).\n\n\nBefore I could do any fancy AI-powered analysis, I needed to get the abstracts and introductions from a large number of academic papers. Getting a bunch of abstracts is straightforward, the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) contain millions of them. However, sourcing the introductions from these papers is much harder. My first idea was to just feed the full-text PDFs from the arXiv dataset directly into the LLM. However, even state-of-the-art models like ChatGPT 4o struggled to extract the abstract and introduction sections I needed. I'm not entirely sure why this task is so difficult, but I think it might something to do with the two-column formatting present in many academic journals. This experience led me to realize that I needed a way to extract text directly from the papers themselves, which led me to web scraping.\n\nWeb scraping is a tedious and annoying task. Small changes between webpages can completely break your scraper, and you're constantly running the risk of getting your IP permanently banned. Journals run by the big publishing companies like Springer and Elsevier also require authentication, which adds an additional layer of complexity. In order to keep things as simple as possible, I chose to extract papers from a single open-source journal[^1], [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/). To actually do the web scraping, I used the Python library [beautifulsoup4](https://pypi.org/project/beautifulsoup4/). I managed to get every paper from PLOS Computational Biology -- a total of $9\\,653$ articles!\n\n[^1]: I picked computational biology over another PLOS journal because its closest to my own research interests. Hopefully, the results of this analysis will come in handy if I ever manage to write a paper!\n\n## Sentence Categorization\n\nAfter gathering the data, I ran each abstract and introduction through [Llama 3.2](https://www.llama.com/) to categorize the sentences. In particular, I used the 90B parameter text-preview model. I was hoping to use the 1B and 3B models which can be run from my laptop, but unfortunately they weren't giving me accurate results. Since I (sadly) don't own an industrial compute cluster, I used the [Groq](https://groq.com/) API to run my analysis on the cloud. With their free plan, I got access to $500\\,000$ tokens of chat completion per day. This turned out to be enough to categorize the abstracts and introductions from about $150$ academic papers each night. I ended up analyzing the data from the $500$ most recent papers over three days. In total, these papers contained over $20\\,000$ sentences. The prompt I used is shown below: \n\n>Here are four categories which can be used to classify the sentences of a scientific paper. Make sure that you correctly divide the text into sentences. Remember that abbreviations like \"et al.\" do not constitute the end of a sentence. However, all sentences must end with a period or question mark.\n>\n>CATEGORY 1: Motivation for the research in a broad context. This type of sentence helps the reader to understand why the research is relevant, useful, and interesting.\n>\n>CATEGORY 2: Past research in the field, which may be experimental or theoretical. Sentences in this category may also describe the results of previous research efforts.\n>\n>CATEGORY 3: Descriptions of topics that remain poorly understood or descriptions of gaps in the literature that need to filled.\n>\n>CATEGORY 4: The methods used by the authors in this research paper. Sentences in this category may also discuss the results of this research paper.\n>\n>A sentence can only belong to one category. Use the context of the paragraph to determine whether a sentence is discussing the research paper itself or a previous study. Your response should be a JSON object in the following format:\n>\n>```json\n>{\n>  \"abstract\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ],\n>  \"introduction\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ]\n>}\n>```\n>\n>\n>The \"sentence\" field must contain the exact sentence that was classified. The \"position\" is the relative position of the sentence in the text (i.e. 1 for the first sentence, 2 for the second sentence, etc.). The \"category\" should be one of 1, 2, 3, or 4. Now, I will provide you with an abstract followed by an introduction. Classify each of the sentences. Make sure that the first object in the \"abstract\" and \"introduction\" fields have a position of 1.\n\n**Remark**: While I was running my classification jobs through Groq, I stumbled across the hilariously named and immensely practical `caffeinate` command, which allows you to prevent your computer from sleeping until a specified process completes.\n\n## Results\n\nIn case you didn't read the prompt, here's a quick recap of the four sentence categoires I used for this project:\n\n1. **Motivation**: Sentences that describe why the research is relevant, useful, and interesting to the scientific community at large.\n2. **Background**: The results of past research in the field, which may be experimental or theoretical. \n3. **Research Gaps**: Areas of the field that remain poorly understood.\n4. **Methods & Results**: Summaries of the methods and results from *this* paper.\n\nWithout further ado, here's what I found. For the sake of brevity, I will use the word \"Section\" to refer generally to the abstract and introduction.\n\n![**Figure 1**: Smoothed frequency of each sentence category plotted against relative position in the abstract/introduction. A sentence at position $m$ in a section with $n$ sentences has a relative position of $m/n$. On average, the categories are arranged in order. Categories 2 (background) and 3 (research gaps) overlap significantly. This suggests that authors are interweaving past results with the questions generated by these studies.](category-kde.png)\n\n![**Figure 2**: Boxplot of the number of sentences in each section and category. On average, abstracts have two sentences of motivation, two sentences of background, and one sentence explaining a research gap. However, most of the abstract is dedicated to summarizing the methods and results from the paper. In the introduction, authors only include a few sentences about motivation and research gaps. The majority of the introduction is dedicated to explaining background information, with some room at the end for summarizing the methods and results. ](category-count.png)\n\n![**Figure 3**: Boxplot of the total number of sentences in the abstract and introduction. The median abstract contains nine sentences, with the majority of abstracts containing between 6 and 11 sentences. Introductions tend to have around 35 sentences, although some are significantly longer. ](section-count.png)\n\n**Key Takeaway 1**: A standard abstract in PLOS Computational Biology has two sentences of motivation, followed by 2-4 sentences explaining background information and the research gap the authors aim to fill. The rest of the abstract summarizes the methods and results in the paper.\n\n**Key Takeaway 2**: A standard introduction in PLOS Computational Biology contains a few sentences of motivation, followed by 20-30 sentences (3-4 paragraphs) explaining relevant background information and discussing the research gaps. This is followed by 4-5 sentences (1 paragraph) summarizing the papers methods and results.\n\n**Remark**: For the data anlysis component of this project I tried using the [tidyverse](https://www.tidyverse.org/) for the first time, and it was great! The entire data processing pipeline is just so idiomatic and fun to use. I think that `ggplot2` definitely has the edge over `matplotlib` and `plotly` (the two Python plotting libraries I've tried) for $90\\%$ of projects. From now on I'll be doing my EDA and plotting in R.  \n","srcMarkdownNoYaml":"\n\nAcademic journals require research articles to follow a specific format. First, an 150-300 word abstract provides an big-picture overview of the main ideas contained within the paper. An abstract may also provide a brief summary of relevant background information or methods used in the paper. Following the abstract, authors typically provide a longer introduction section. In this authors may do a number of things, including explain why their research is important, summarize past results in the field, identify key gaps in the literature, and discuss methods used in the paper. After the introduction, authors typically include a methods, results, discussion, and conclusion section. However, these sections are often modified, split up, or removed entirely to better suit the project.\n\nSince the abstract and introduction are crucial to any research paper, understanding how to write these sections effectively is an essential skill for a researcher. Advice on paper writing is typically *qualitative* -- stuff like \"start broad and then gradually get narrower\" or \"make sure to emphasize the importance of your research\". Today, I outline a *quantitative* framework for writing abstracts and introduction. As an example, my analysis gives an estimate of how many sentences of motivation you should provide and how where these sentences should be located in your paper. To do this, I analyzed over $9\\,000$ papers from the [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/) journal using the open source LLM [Llama 3.2](https://www.llama.com/).\n \n**Remark**: The idea for this project came from my supervisor, [Prof. Eric Cytrynbaum](https://personal.math.ubc.ca/~cytryn/index.shtml).\n\n## Web Scraping\n\nBefore I could do any fancy AI-powered analysis, I needed to get the abstracts and introductions from a large number of academic papers. Getting a bunch of abstracts is straightforward, the [arXiv Dataset](https://www.kaggle.com/datasets/Cornell-University/arxiv) contain millions of them. However, sourcing the introductions from these papers is much harder. My first idea was to just feed the full-text PDFs from the arXiv dataset directly into the LLM. However, even state-of-the-art models like ChatGPT 4o struggled to extract the abstract and introduction sections I needed. I'm not entirely sure why this task is so difficult, but I think it might something to do with the two-column formatting present in many academic journals. This experience led me to realize that I needed a way to extract text directly from the papers themselves, which led me to web scraping.\n\nWeb scraping is a tedious and annoying task. Small changes between webpages can completely break your scraper, and you're constantly running the risk of getting your IP permanently banned. Journals run by the big publishing companies like Springer and Elsevier also require authentication, which adds an additional layer of complexity. In order to keep things as simple as possible, I chose to extract papers from a single open-source journal[^1], [PLOS Computational Biology](https://journals.plos.org/ploscompbiol/). To actually do the web scraping, I used the Python library [beautifulsoup4](https://pypi.org/project/beautifulsoup4/). I managed to get every paper from PLOS Computational Biology -- a total of $9\\,653$ articles!\n\n[^1]: I picked computational biology over another PLOS journal because its closest to my own research interests. Hopefully, the results of this analysis will come in handy if I ever manage to write a paper!\n\n## Sentence Categorization\n\nAfter gathering the data, I ran each abstract and introduction through [Llama 3.2](https://www.llama.com/) to categorize the sentences. In particular, I used the 90B parameter text-preview model. I was hoping to use the 1B and 3B models which can be run from my laptop, but unfortunately they weren't giving me accurate results. Since I (sadly) don't own an industrial compute cluster, I used the [Groq](https://groq.com/) API to run my analysis on the cloud. With their free plan, I got access to $500\\,000$ tokens of chat completion per day. This turned out to be enough to categorize the abstracts and introductions from about $150$ academic papers each night. I ended up analyzing the data from the $500$ most recent papers over three days. In total, these papers contained over $20\\,000$ sentences. The prompt I used is shown below: \n\n>Here are four categories which can be used to classify the sentences of a scientific paper. Make sure that you correctly divide the text into sentences. Remember that abbreviations like \"et al.\" do not constitute the end of a sentence. However, all sentences must end with a period or question mark.\n>\n>CATEGORY 1: Motivation for the research in a broad context. This type of sentence helps the reader to understand why the research is relevant, useful, and interesting.\n>\n>CATEGORY 2: Past research in the field, which may be experimental or theoretical. Sentences in this category may also describe the results of previous research efforts.\n>\n>CATEGORY 3: Descriptions of topics that remain poorly understood or descriptions of gaps in the literature that need to filled.\n>\n>CATEGORY 4: The methods used by the authors in this research paper. Sentences in this category may also discuss the results of this research paper.\n>\n>A sentence can only belong to one category. Use the context of the paragraph to determine whether a sentence is discussing the research paper itself or a previous study. Your response should be a JSON object in the following format:\n>\n>```json\n>{\n>  \"abstract\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ],\n>  \"introduction\": [\n>    {\n>      \"sentence\": ...,\n>      \"position\": ...,\n>      \"category\": ...,\n>    },\n>    ...\n>  ]\n>}\n>```\n>\n>\n>The \"sentence\" field must contain the exact sentence that was classified. The \"position\" is the relative position of the sentence in the text (i.e. 1 for the first sentence, 2 for the second sentence, etc.). The \"category\" should be one of 1, 2, 3, or 4. Now, I will provide you with an abstract followed by an introduction. Classify each of the sentences. Make sure that the first object in the \"abstract\" and \"introduction\" fields have a position of 1.\n\n**Remark**: While I was running my classification jobs through Groq, I stumbled across the hilariously named and immensely practical `caffeinate` command, which allows you to prevent your computer from sleeping until a specified process completes.\n\n## Results\n\nIn case you didn't read the prompt, here's a quick recap of the four sentence categoires I used for this project:\n\n1. **Motivation**: Sentences that describe why the research is relevant, useful, and interesting to the scientific community at large.\n2. **Background**: The results of past research in the field, which may be experimental or theoretical. \n3. **Research Gaps**: Areas of the field that remain poorly understood.\n4. **Methods & Results**: Summaries of the methods and results from *this* paper.\n\nWithout further ado, here's what I found. For the sake of brevity, I will use the word \"Section\" to refer generally to the abstract and introduction.\n\n![**Figure 1**: Smoothed frequency of each sentence category plotted against relative position in the abstract/introduction. A sentence at position $m$ in a section with $n$ sentences has a relative position of $m/n$. On average, the categories are arranged in order. Categories 2 (background) and 3 (research gaps) overlap significantly. This suggests that authors are interweaving past results with the questions generated by these studies.](category-kde.png)\n\n![**Figure 2**: Boxplot of the number of sentences in each section and category. On average, abstracts have two sentences of motivation, two sentences of background, and one sentence explaining a research gap. However, most of the abstract is dedicated to summarizing the methods and results from the paper. In the introduction, authors only include a few sentences about motivation and research gaps. The majority of the introduction is dedicated to explaining background information, with some room at the end for summarizing the methods and results. ](category-count.png)\n\n![**Figure 3**: Boxplot of the total number of sentences in the abstract and introduction. The median abstract contains nine sentences, with the majority of abstracts containing between 6 and 11 sentences. Introductions tend to have around 35 sentences, although some are significantly longer. ](section-count.png)\n\n**Key Takeaway 1**: A standard abstract in PLOS Computational Biology has two sentences of motivation, followed by 2-4 sentences explaining background information and the research gap the authors aim to fill. The rest of the abstract summarizes the methods and results in the paper.\n\n**Key Takeaway 2**: A standard introduction in PLOS Computational Biology contains a few sentences of motivation, followed by 20-30 sentences (3-4 paragraphs) explaining relevant background information and discussing the research gaps. This is followed by 4-5 sentences (1 paragraph) summarizing the papers methods and results.\n\n**Remark**: For the data anlysis component of this project I tried using the [tidyverse](https://www.tidyverse.org/) for the first time, and it was great! The entire data processing pipeline is just so idiomatic and fun to use. I think that `ggplot2` definitely has the edge over `matplotlib` and `plotly` (the two Python plotting libraries I've tried) for $90\\%$ of projects. From now on I'll be doing my EDA and plotting in R.  \n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"litera","title-block-banner":true,"title":"A Structural Analysis of Academic Writing","date":"2024-11-05","date-modified":"2024-11-05","categories":["LLM","Python","R","Web Scraping"],"image":"sunset5.jpeg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}